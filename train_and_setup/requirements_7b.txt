# LLaMA 3.2 7B Training Dependencies
# Optimized for SageMaker with memory management

# Core PyTorch with CUDA support
torch>=2.0.0
torchvision
torchaudio
--index-url https://download.pytorch.org/whl/cu118

# Transformers and related libraries
transformers==4.35.0
datasets>=3.4.1,<4.0.0
huggingface_hub>=0.34.0
tokenizers>=0.14.0

# Training libraries
accelerate>=0.20.0
peft>=0.4.0
trl>=0.7.0

# Quantization and optimization
bitsandbytes>=0.41.0
xformers>=0.0.20
triton>=2.0.0

# Memory and system monitoring
psutil>=5.8.0
tqdm>=4.64.0

# Unsloth for optimized training
unsloth>=2024.1.0
unsloth_zoo>=2024.1.0

# Additional utilities
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
seaborn>=0.11.0

# For data processing
pyarrow>=10.0.0
fsspec>=2023.1.0

# For logging and monitoring
wandb>=0.15.0
tensorboard>=2.10.0

# For web deployment (if needed)
flask>=2.0.0
streamlit>=1.20.0
requests>=2.28.0

# For API integration
pydantic>=1.10.0
fastapi>=0.95.0
uvicorn>=0.20.0
