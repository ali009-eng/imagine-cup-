# LLaMA 3.2 7B MIMIC-IV-ED Multi-GPU Training

## üöÄ Enhanced Training for LLaMA 3.2 7B with MIMIC-IV-ED Dataset

This repository contains optimized training scripts for fine-tuning LLaMA 3.2 7B on **MIMIC-IV-ED** (Medical Information Mart for Intensive Care IV - Emergency Department) data using **multi-GPU T4** instances for enhanced performance.

## üìã Features

### üéØ Model Specifications
- **Base Model**: LLaMA 3.2 7B (unsloth/llama-3.2-7b-bnb-4bit)
- **Dataset**: MIMIC-IV-ED (Real clinical emergency department data)
- **Quantization**: 4-bit quantization for memory efficiency
- **LoRA Configuration**: r=16, alpha=32 (optimized for 7B)
- **Sequence Length**: 2048 tokens
- **Training Method**: Supervised Fine-Tuning (SFT)

### üöÄ Multi-GPU Optimizations
- **Multi-GPU Support**: Automatic distribution across multiple T4 GPUs
- **NCCL Backend**: Optimized communication between GPUs
- **Memory Distribution**: 80% memory usage per GPU
- **Gradient Checkpointing**: Enabled for memory efficiency
- **CPU Offloading**: Automatic device mapping for large models

### üìä MIMIC-IV-ED Dataset Features
- **Real Clinical Data**: Actual emergency department triage scenarios
- **Triage Acuity Levels**: 1-5 scale (1=highest severity, 5=lowest)
- **Vital Signs**: Temperature, heart rate, respiratory rate, oxygen saturation, blood pressure
- **Chief Complaints**: Patient-reported reasons for ED visit
- **Pain Assessment**: Patient-reported pain levels
- **Clinical Context**: Rich medical information for better training

### üíæ Memory Management
- **Per-GPU Memory**: 80% utilization per T4 GPU
- **Total Memory**: Minimum 25GB recommended across all GPUs
- **Batch Size**: 2 (distributed across GPUs)
- **Gradient Accumulation**: 16 steps for effective batch size
- **Chunk Processing**: 1500 samples per chunk
- **Automatic Cleanup**: Memory clearing between chunks

## üìÅ File Structure

```
train_and_setup/
‚îú‚îÄ‚îÄ sagemaker_7b_mimic_training.py      # Main MIMIC-ED 7B training script
‚îú‚îÄ‚îÄ sagemaker_mimic_multi_gpu_setup.py  # Multi-GPU setup script
‚îú‚îÄ‚îÄ requirements_7b.txt                  # Dependencies for 7B
‚îú‚îÄ‚îÄ README_MIMIC_MULTI_GPU.md           # This file
‚îú‚îÄ‚îÄ mimic_ed_data.csv                   # Your MIMIC-IV-ED dataset
‚îî‚îÄ‚îÄ llama_7b_mimic_checkpoints/         # Training checkpoints
```

## üõ†Ô∏è Setup Instructions

### 1. Multi-GPU Environment Setup

```bash
# Run the multi-GPU setup script
python sagemaker_mimic_multi_gpu_setup.py
```

### 2. MIMIC-IV-ED Data Preparation

#### Data Source
- **Download**: MIMIC-IV-ED from [PhysioNet](https://physionet.org/content/mimic-iv-ed/2.2/)
- **Credentials**: Requires completion of data use agreement
- **License**: PhysioNet Credentialed Health Data License 1.5.0

#### Required Tables
1. **edstays**: Patient stay information
2. **triage**: Triage data with acuity levels (1-5)
3. **vitalsign**: Vital signs during ED stay
4. **diagnosis**: ICD diagnosis codes

#### Data Processing
Create a combined CSV file `mimic_ed_data.csv` with these columns:
```csv
acuity,chiefcomplaint,temperature,heartrate,resprate,o2sat,sbp,dbp,pain
1,"chest pain, shortness of breath",37.2,95,18,98,140,85,8
2,"fever, headache",38.5,88,16,96,135,82,6
...
```

### 3. Start Training

```bash
# Run the MIMIC-ED 7B training script
python sagemaker_7b_mimic_training.py
```

## ‚öôÔ∏è Configuration

### Key Parameters for Multi-GPU 7B Model

```python
CONFIG = {
    'BASE_MODEL_NAME': "unsloth/llama-3.2-7b-bnb-4bit",
    'HF_MODEL_ID': "ali009eng/llama-7b-mimic-ed-triage",  # New MIMIC repo
    'CHUNK_SIZE': 1500,                    # Optimized for MIMIC data
    'BATCH_SIZE': 2,                       # Multi-GPU batch size
    'LEARNING_RATE': 5e-5,                # Lower learning rate for 7B
    'GRADIENT_ACCUMULATION_STEPS': 16,     # Reduced for multi-GPU
    'GRADIENT_CHECKPOINTING': True,        # Enabled for memory efficiency
    'CPU_OFFLOAD': True,                   # Enable CPU offloading
    'MULTI_GPU': True,                     # Enable multi-GPU
    'DDP_BACKEND': "nccl",                 # NCCL backend for multi-GPU
}
```

### Multi-GPU Settings

```python
# Training arguments for multi-GPU
training_args = TrainingArguments(
    per_device_train_batch_size=2,         # Batch size per GPU
    gradient_accumulation_steps=16,        # Effective batch size = 2 * 16 = 32
    ddp_backend="nccl",                    # NCCL backend
    dataloader_drop_last=True,             # Important for multi-GPU
    dataloader_num_workers=2,              # Increased for multi-GPU
)
```

## üìä Training Process

### 1. Data Processing
- **Validation**: Remove rows missing essential fields (acuity, chiefcomplaint)
- **Augmentation**: Create variations for better generalization
- **Formatting**: Enhanced prompts with MIMIC clinical context
- **Splitting**: 85% train, 15% validation

### 2. Model Loading
- **Base Model**: Load LLaMA 3.2 7B with 4-bit quantization
- **Multi-GPU Distribution**: Automatic distribution across available GPUs
- **LoRA Adapters**: Apply optimized LoRA configuration
- **Device Mapping**: Automatic CPU/GPU distribution

### 3. Training Loop
- **Chunk Processing**: Process data in 1500-sample chunks
- **Multi-GPU Monitoring**: Track memory usage across all GPUs
- **Checkpointing**: Save every 150 steps
- **Early Stopping**: Stop if no improvement for 3 epochs

### 4. Model Saving
- **Local Checkpoints**: Save to `./llama_7b_mimic_checkpoints/`
- **HuggingFace Upload**: Automatic upload to new MIMIC repository
- **Metadata**: Save training metrics and MIMIC configuration

## üîç Monitoring

### Multi-GPU Memory Usage
- **Per-GPU Monitoring**: Memory usage for each GPU every 15 steps
- **Total Memory**: Combined memory usage across all GPUs
- **CPU Memory**: Continuous CPU memory tracking
- **Automatic Cleanup**: Memory clearing between chunks

### Training Metrics
- **Loss**: Training and validation loss
- **Progress**: Percentage completion
- **GPU Utilization**: Per-GPU utilization rates
- **Memory Efficiency**: Memory usage optimization

### Logs
- **File**: `./llama_7b_mimic_checkpoints/training.log`
- **Console**: Real-time multi-GPU progress updates
- **Errors**: Detailed error reporting with GPU context

## ‚ö†Ô∏è Important Notes

### Hardware Requirements
- **GPU Type**: T4 GPUs recommended (other types supported)
- **GPU Count**: 2+ GPUs for optimal performance
- **Total GPU Memory**: Minimum 25GB across all GPUs
- **CPU Memory**: 32GB+ recommended
- **Storage**: 100GB+ for MIMIC checkpoints

### Training Time
- **Per Chunk**: ~1.5-3 hours (depending on GPU count)
- **Full Dataset**: ~15-30 hours for typical MIMIC-IV-ED size
- **Multi-GPU Speedup**: ~1.5-2x faster than single GPU
- **Resume**: Can resume from any checkpoint

### Cost Considerations
- **SageMaker**: Higher cost than single GPU
- **GPU Hours**: ~1.5-2x more efficient than single GPU
- **Storage**: More checkpoints and larger model
- **MIMIC Data**: Requires PhysioNet credentials

## üöÄ Deployment

### HuggingFace Model
- **Repository**: `ali009eng/llama-7b-mimic-ed-triage`
- **Access**: Public repository
- **Versioning**: Automatic version control
- **Dataset**: MIMIC-IV-ED specific

### Local Usage
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("unsloth/llama-3.2-7b-bnb-4bit")
tokenizer = AutoTokenizer.from_pretrained("unsloth/llama-3.2-7b-bnb-4bit")

# Load MIMIC-ED LoRA adapters
model = PeftModel.from_pretrained(base_model, "ali009eng/llama-7b-mimic-ed-triage")
```

## üîß Troubleshooting

### Common Multi-GPU Issues

1. **NCCL Communication Errors**
   - Check GPU interconnect
   - Verify NCCL installation
   - Set `NCCL_DEBUG=INFO`

2. **Memory Imbalance**
   - Reduce batch size per GPU
   - Enable gradient checkpointing
   - Use CPU offloading

3. **Training Instability**
   - Reduce learning rate
   - Increase warmup steps
   - Check data quality

### Performance Tips

1. **Multi-GPU Optimization**
   - Use NCCL backend
   - Optimize batch size per GPU
   - Monitor GPU utilization

2. **Memory Management**
   - Enable gradient checkpointing
   - Use CPU offloading
   - Monitor memory per GPU

## üìà Expected Results

### Performance Metrics
- **Accuracy**: 90-95% on MIMIC-IV-ED triage classification
- **Training Time**: 15-30 hours for full dataset
- **Model Size**: ~4GB (4-bit quantized)
- **Multi-GPU Speedup**: 1.5-2x faster than single GPU

### Quality Improvements
- **Clinical Accuracy**: Real medical triage scenarios
- **Medical Knowledge**: Enhanced medical domain understanding
- **Consistency**: More consistent triage decisions
- **Generalization**: Better handling of diverse medical cases

## ü§ù Support

For issues or questions:
1. Check the troubleshooting section
2. Review the logs in `./llama_7b_mimic_checkpoints/training.log`
3. Monitor multi-GPU memory usage
4. Verify MIMIC-IV-ED data format

## üìù License and Compliance

### MIMIC-IV-ED Data
- **Source**: PhysioNet Credentialed Health Data
- **License**: PhysioNet Credentialed Health Data License 1.5.0
- **Compliance**: Must complete data use agreement
- **Usage**: Educational and research purposes only

### Model License
- **Base Model**: LLaMA 3.2 license terms apply
- **Fine-tuned Model**: Educational and research purposes
- **Commercial Use**: Check base model license terms

## üîó Additional Resources

- **MIMIC-IV-ED**: [PhysioNet Documentation](https://physionet.org/content/mimic-iv-ed/2.2/)
- **PhysioNet**: [Data Access and Credentials](https://physionet.org/)
- **Multi-GPU Training**: [PyTorch DDP Guide](https://pytorch.org/docs/stable/notes/ddp.html)
- **NCCL**: [NVIDIA Collective Communications](https://developer.nvidia.com/nccl)
